Annotation on reader's perception elements using LLMs
=====================================================

For my bachelor's thesis, I am doing research on the effect of reader's perception
elements on persuasion. For this, we need to annotate a large dataset of Reddit
r/ChangeMyView comments. We have annotated ~600 comments manually and will use
this code to annotate the full dataset (or at least a large part of it).

This GitHub repository contains the code used to get the annotations from the LLMs and
further processing of the annotations.

Description of files
====================

- `main.py`
    - contains the code that interfaces with the LLM, the system prompt, does the annotations
      and saves those annotations to a file.
- `validate_annotations.py`
    - validates the annotations and outputs how many annotations were properly made.
- `evaluation.py`
    - validates the LLM annotation file created by `main.py` against a golden standard file
- `test_fewshot.sh`
    - runs 10 runs of few-shot annotation on 1, 3, 5, 8 and 10 examples. It also validates the annotations.
- `evaluate_fewshot.sh`
    - takes the annotations from `test_fewshot.sh`, runs them through `evaluation.py` to get scores, stores
    the scores temporarily and runs `parse_bulk_eval.py` to get the mean F1-score and standard deviation.
- `parse_bulk_eval.py`
    - takes the scores from the text files generated by `evaluate_fewshot.sh` and calculates the mean F1-score
    and its standard deviation. This program should not be run directly.
- `link_annotations.py`
    - takes the annotations generated by `main.py` and adds them to the dataset. It also discards any comments that are useless,
    such as deleted comments (body: "[deleted]") and empty comments.
- `convert_selftext_to_body.py`
    - some comments had an empty body and had their text inside the `selftext` attribute. To fix this, after annotating the initial
      dataset, a new partial dataset can be generated with this program which after annotation and linking, can be combined with
      the first dataset using `data_combiner.py`.
    - **Note**: before linking the dataset created by this program after annotation, please run `fix_selftext_missing.py`, otherwise
      `link_annotations.py` will discard the comment as being 'useless'.
- `data_combiner.py`
    - combines two datasets together into one file.
- `duplicate_check.py`
    - removes duplicate comments from the dataset.
- `count_comments.py`
    - counts the number of comments and posts in a dataset, either a JSONL dataset or a JSON file with an array of JSON objects
      for every comment.
- `fix_selftext_missing.py`
    - adds an empty selftext attribute to comments that do not have a selftext attribute. `convert_selftext_to_body.py` removes
      them, but this causes these comments to be discarded by `link_annotations.py` as being useless.
- `preprocess.py`
    - intended more for reference: this program builds the annotation dataset which can be annotated by human annotators to
      produce a golden standard for use with the model.

Usage
=====

This usage guide will assume a simple annotation procedure using few-shot, 3 examples on a binary scale.

Annotate data into a JSON format. The program expects the body of the comment (the title of a post should be
merged into this body), the 'name' of the comment and the classes, as well as a selftext attribute.

## Classes

- `story_class`: Either 'Story' or 'Not Story', depending if there is a story in the comment or not.
- `suspense`: Likert scale rating from 1 to 5.
- `curiosity`: Likert scale rating from 1 to 5.
- `surprise`: Likert scale rating from 1 to 5.

## Ollama

Download and install [Ollama](https://ollama.com). You do not have to install it system-wide but make sure
it is in your `$PATH` environment variable.

Run it with the model you want to use (in our research we used `llama3.2`):

    $ ollama run llama3.2

It will download the model and allow you to chat with the model. Just close the prompt with Ctrl+D.
Start the Ollama server:

    $ ollama serve

Ollama is now ready to answer LLM prompts from our program.

## Setting up Python

Our annotation program was developed with and tested on Python 3.11 and Python 3.13. It might not
work on older or newer versions of Python.

### Setting up the Python virtual environment

Create a Python virtual environment, activate it and install the required packages:

    $ python3 -m venv .venv/
    $ source .venv/bin/activate
    $ pip install -r requirements.txt

## Workflow

In this example, we will annotate the test set and use the training set to provide examples for few-shot annotations.

Make sure you have a golden standard or another annotated dataset that you want to annotate and can provide examples from.
It is not recommended to use the same file to evaluate and provide few-shot annotations at the same time.

In another terminal (tmux is recommended, since you can create multiple windows), you can run `main.py` to do the annotations:

    $ python3 main.py <input file> <method> <output file> <fewshot example count> <scale> <fewshot example file>

For our use case, we will use the golden standard and few-shot-3. Note that scale is referred to as "rp style" by the program.
We will run:

    $ python3 main.py data/golden-standard-test.json fewshot output.json 3 binary data/golden-standard-train.json

Let's validate the annotations with `validate_annotations.py`:

    $ python3 validate_annotations.py output.json

If it looks good, we can evaluate the model performance:

    $ python3 evaluation.py output.json data/golden-standard-test.json

Make sure when evaluating, you let the model annotate that golden standard file first, before evaluating against it!

That's enough for basic model usage and annotation. But we want the dataset to be annotated. Let's propose I just let it annotate my
dataset, 'dataset.json' and I want to add its annotations to my dataset. To do this, a few more steps are required.
First, we obtain the selftext dataset:

    $ python3 convert_selftext_to_body.py dataset.json dataset_selftext.json

We annotate that dataset:

    $ python3 main.py dataset_selftext.json fewshot output_selftext.json 3 binary data/golden-standard-train.json
    $ python3 validate_annotations.py output_selftext.json

We fix the selftext attribute:

    $ python3 fix_selftext_missing.py dataset_selftext.json

We link our annotations:

    $ python3 link_annotations.py dataset.json output.json dataset_annotated.json
    $ python3 link_annotations.py dataset_selftext.json output_selftext.json dataset_selftext_annotated.json

Finally, we can combine them into one dataset:

    $ python3 data_combiner.py dataset_annotated.json dataset_selftext_annotated.json dataset_full_annotated.json

Now, we have our dataset completely annotated! Most of the time, you should now be able to start conducting analysis
on your dataset. Sometimes, there are accidental duplicates created, either by weird datasets or because of errors
in our program. To fix this, just run `duplicate_check.py` on your new dataset to remove the duplicates. Now you can
really start analyzing.

# Notice

This is the repository as of finishing the thesis. I might improve this repository with easier to use programs
and fewer scripts in the future, to improve reproducibility.